## Redis

### Redis基础

#### Redis数据结构

Redis的5个基本数据类型：**String**、**Hash**、**List**、**Set**、**SortedSet**。

这5个数据类型对应的底层数据结构实现如图

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sMGBkAV80RrhbpyZ4PgqGCIJmEp8Bx7xFPuX3G4Hw9YiaCuL9mJPyxJicg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### String

Redis中的String键值都是字符串（最大能存储512MB的数据）

Redis 是用 C 语言实现的，但是它没有直接使用 C 语言的 char* 字符数组来实现字符串，而是自己封装了一个名为简单动态字符串：SDS。

###### C语言字符数组

C 语言的字符串其实就是一个字符数组，即数组中每个元素是字符串中的一个字符。在 C 语言里，对字符串操作时，char * 指针只是指向字符数组的起始位置，而**字符数组的结尾位置就用“\0”表示，意思是指字符串的结束**。

所以，C语言字符数组可以优化的地方：

- C 语言获取字符串长度操作的时间复杂度是 O（N）
- 不能使用\0
- 必须是AscII码，不能保存图片、视频、音频等数据
- 不安全，指针容易造成内存溢出

###### SDS

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sM6xZ3dRtuV7BUXW4RmKicKnqk6iaibzxISpRjuOel2qWOvo1xych25TsGA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

优点：

- O（1）复杂度获取字符串长度。

-  SDS 不需要用 “\0” 字符来标识字符串结尾了，而且 SDS 的 API 都是以处理二进制的方式来处理 SDS 存放在 buf[] 里的数据，数据更安全，不会发生溢出

- 通过使用二进制安全的 SDS，而不是 C 字符串，使得 Redis 不仅 可以保存文本数据，也可以保存任意格式的二进制数据。

- Redis 的 SDS 结构里引入了 alloc 和 leb 成员变量，这样 SDS API 通过 `alloc - len` 计算，可以算出剩余可用的空间大小，这样在对字符串做修改操作的时候，就可以由程序内部判断缓冲区大小是否足够用。

  而且，当判断出缓冲区大小不够用时，Redis 会自动将扩大 SDS 的空间大小，以满足修改所需的大小。

  在扩展 SDS 空间之前，SDS API 会优先检查未使用空间是否足够，如果不够的话，API 不仅会为 SDS 分配修改所必须要的空间，还会给 SDS 分配额外的「未使用空间」。

  这样的好处是，下次在操作 SDS 时，如果 SDS 空间够的话，API 就会直接使用「未使用空间」，而无须执行内存分配，有效的减少内存分配次数。

- 通过 flags 成员变量，限制字符数组的长度和分配空间大小。灵活保存不同大小的字符串，同时根据字节对齐等手段，有效节省内存空间。

##### 双向链表

Redis内的List为双向链表，同时还包含一个list数据结构，使操作链表更方便。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sM6HjDC6rAqPcYg9jh886SqrJDOjHib2bPKr3ADKiaySRhr5szOKy8LZDg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- listNode 链表节点带有 prev 和 next 指针，**获取某个节点的前置节点或后置节点的时间复杂度只需O(1)，而且这两个指针都可以指向 NULL，所以链表是无环链表**；
- list 结构因为提供了表头指针 head 和表尾节点 tail，所以**获取链表的表头节点和表尾节点的时间复杂度只需O(1)**；
- list 结构因为提供了链表节点数量 len，所以**获取链表中的节点数量的时间复杂度只需O(1)**；
- listNode 链表节使用 void* 指针保存节点值，并且可以通过 list 结构的 dup、free、match 函数指针为节点设置该节点类型特定的函数，因此链表节点可以保存各种不同类型的值；

链表的缺陷也是有的，链表每个节点之间的内存都是不连续的，意味着无法很好利用 CPU 缓存。

能很好利用 CPU 缓存的数据结构就是数组，因为数组的内存是连续的，这样就可以充分利用 CPU 缓存来加速访问。

因此，Redis 的 list 数据类型在数据量比较少的情况下，会采用「压缩列表」作为底层数据结构的实现，压缩列表就是由数组实现的。

##### 压缩列表

压缩列表是 Redis 数据类型为 list 和 hash 的底层实现之一。

- 当一个列表键（list）只包含少量的列表项，并且每个列表项都是小整数值，或者长度比较短的字符串，那么 Redis 就会使用压缩列表作为列表键（list）的底层实现。
- 当一个哈希键（hash）只包含少量键值对，并且每个键值对的键和值都是小整数值，或者长度比较短的字符串，那么 Redis 就会使用压缩列表作为哈希键（hash）的底层实现。

压缩列表是 Redis 为了节约内存而开发的，它是**由连续内存块组成的顺序型数据结构**，有点类似于数组。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sMg5WvbL2XCs8b3D0icbLkm0Wiay5ptOsr8agkI6POzCVvIYL731d8CQhQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- zlbytes，记录整个压缩列表占用对内存字节数；
- zltail，记录压缩列表「尾部」节点距离起始地址由多少字节，也就是列表尾的偏移量；
- zllen，记录压缩列表包含的节点数量；
- zlend，标记压缩列表的结束点，特殊值 OxFF（十进制255）。

压缩列表节点包含三部分内容：

- prevlen，记录了前一个节点的长度；
- encoding，记录了当前节点实际数据的类型以及长度；
- data，记录了当前节点的实际数据；

当我们往压缩列表中插入数据时，压缩列表 就会根据数据是字符串还是整数，以及它们的大小会在 prevlen 和 encoding 这两个元素里保存不同的信息，这种根据数据大小进行对应信息保存的设计思想，正是 Redis 为了节省内存而采用的。

###### 连锁更新

压缩列表除了查找复杂度高的问题，压缩列表在插入元素时，如果内存空间不够了，压缩列表还需要重新分配一块连续的内存空间，而这可能会引发**连锁更新**的问题。

压缩列表里的每个节点中的  prevlen 属性都记录了「前一个节点的长度」，而且 prevlen 属性的空间大小跟前一个节点长度值有关，比如：

- 如果前一个**节点的长度小于 254 字节**，那么 prevlen 属性需要用 **1 字节的空间**来保存这个长度值；
- 如果前一个**节点的长度大于等于 254 字节**，那么 prevlen 属性需要用 **5 字节的空间**来保存这个长度值；

现在假设一个压缩列表中有多个连续的、长度在 250～253 之间的节点。

因为这些节点长度值小于 254 字节，所以 prevlen 属性需要用 1 字节的空间来保存这个长度值。

这时，如果将一个长度大于等于 254 字节的新节点加入到压缩列表的表头节点，即新节点将成为 e1 的前置节点，如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sMicJqJsfs16PEDf1huAA1HSuJGLQVj1jECEvnibbzhnYJS9QuDs1JC2cw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因为 e1 节点的 prevlen 属性只有 1 个字节大小，无法保存新节点的长度，此时就需要对压缩列表的空间重分配操作，并将 e1 节点的 prevlen 属性从原来的 1 字节大小扩展为 5 字节大小。

多米诺牌的效应就此开始。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sMvXGUHrVruHAIgBVzm0pMibyJbboLjGd7OeQxmHbHuZRUBQ3As52ufZw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

e1 原本的长度在 250～253 之间，因为刚才的扩展空间，此时 e1 的长度就大于等于 254 了，因此原本 e2 保存 e1 的 prevlen 属性也必须从 1 字节扩展至 5 字节大小。

正如扩展 e1 引发了对 e2 扩展一样，扩展 e2 也会引发对 e3 的扩展，而扩展 e3 又会引发对 e4 的扩展…. 一直持续到结尾。

**这种在特殊情况下产生的连续多次空间扩展操作就叫做「连锁更新」**，就像多米诺牌的效应一样，第一张牌倒下了，推动了第二张牌倒下；第二张牌倒下，又推动了第三张牌倒下….

连锁更新一旦发生，就会导致压缩列表 占用的内存空间要多次重新分配，这就会直接影响到压缩列表的访问性能。

所以说，虽然压缩列表紧凑型的内存布局能节省内存开销，但是如果保存的元素数量增加了，或是元素变大了，压缩列表就会面临「连锁更新」的风险。

因此，**压缩列表只会用于保存的节点数量不多的场景**，只要节点数量足够小，即使发生连锁更新，也是能接受的。

Redis在3.2后更新了quicklist，是压缩列表和双向链表的组合结构，本身是一个链表，链表的每一个元素是压缩列表。list底层实现变为quicklist

Redis在7.0后更新了紧凑列表（Listpack），解决了压缩列表的级联更新问题。list底层实现变为双向链表+Listpack

Listpack去除了prevlen，改为当前节点的长度，从而避免了级联更新。

##### 哈希表

哈希表是一种保存键值对（key-value）的数据结构。

哈希表中的每一个 key 都是独一无二的，程序可以根据 key 查找到与之关联的 value，或者通过 key 来更新 value，又或者根据 key 来删除整个 key-value等等。

当一个哈希键包含的 key-value 比较多，或者 key-value 中元素都是比较长多字符串时，Redis 就会使用哈希表作为哈希键的底层实现。

Hash 表优点在于，它**能以 O(1) 的复杂度快速查询数据**。主要是通过 Hash 函数的计算，就能定位数据在表中的位置，紧接着可以对数据进行操作，这就使得数据操作非常快。

但是存在的风险也是有，在哈希表大小固定的情况下，随着数据不断增多，那么**哈希冲突**的可能性也会越高。

解决哈希冲突的方式，有很多种。**Redis 采用了链式哈希**，在不扩容哈希表的前提下，将具有相同哈希值的数据链接起来，以便这些数据在表中仍然可以被查询到。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sM7nfaVwHPw2KKeLeHaJ0ntL8oXR5CtJr6FaGMNOB2jPMiaumbP5T2Sqw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

不过，链式哈希局限性也很明显，随着链表长度的增加，在查询这一位置上的数据的耗时就会增加，毕竟链表的查询的时间复杂度是 O（n）。

要想解决这一问题，就需要进行 rehash，就是对哈希表的大小进行扩展。

###### Rehash

Redis 会使用了两个全局哈希表进行 rehash。

为了避免 rehash 在数据迁移过程中，因拷贝数据的耗时，影响 Redis 性能的情况，所以 Redis 采用了**渐进式 rehash**，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移。

渐进式 rehash 步骤如下：

- 给「哈希表 2」 分配空间，通常为原表大小的2倍（实际通常为已有key数量的2倍的最近的2次幂）；
- **在 rehash 进行期间，每次哈希表元素进行新增、删除、查找或者更新操作时，Redis 除了会执行对应的操作之外，还会顺序将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上**（通过rehashidx控制，每操作一次rehashidx+1，全部rehash完后rehashidx变为1）；
- 随着处理客户端发起的哈希表操作请求数量越多，最终会把「哈希表 1 」的所有 key-value 迁移到「哈希表 2」，从而完成 rehash 操作。

这样就巧妙地把一次性大量数据迁移工作的开销，分摊到了多次处理请求的过程中，避免了一次性 rehash 的耗时操作。

在进行渐进式 rehash 的过程中，会有两个哈希表，所以在渐进式 rehash 进行期间，哈希表元素的删除、查找、更新等操作都会在这两个哈希表进行。

比如，查找一个 key 的值的话，先会在哈希表 1 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。

另外，在渐进式 rehash 进行期间，新增一个 key-value 时，会被保存到「哈希表 2 」里面，而「哈希表 1」 则不再进行任何添加操作，这样保证了「哈希表 1 」的 key-value 数量只会减少，随着 rehash 操作的完成，最终「哈希表 1 」就会变成空表。

###### Rehash的触发条件

rehash 的触发条件跟**负载因子（load factor）**有关系。

负载因子可以通过下面这个公式计算：

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfm9oXYxAKmUq9ezu14J4sM1NOxAqicabDlqYLvgaG6NRnVpLejt5licwqTNF6o6euzgfibj79qVqC8w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

触发 rehash 操作的条件，主要有两个：

- **当负载因子大于等于 1 ，并且 Redis 没有在执行 bgsave 命令或者 bgrewiteaof 命令，也就是没有执行 RDB 快照或没有进行 AOF 重写的时候，就会进行 rehash 操作。**
- **当负载因子大于等于 5 时，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作。**
- 当负载因子小于0.1，会进行缩容操作，新表的大小是老表已使用的最近的2的幂次

##### 跳表

因为 zset 要支持随机的插入和删除，所以它 **不宜使用数组来实现**，不使用平衡树/红黑树：

1. **性能考虑：** 在高并发的情况下，树形结构需要执行一些类似于 rebalance 这样的可能涉及整棵树的操作，相对来说跳跃表的变化只涉及局部
2. **实现考虑：** 在复杂度与红黑树相同的情况下，跳跃表实现起来更简单，看起来也更加直观；

**跳跃表 skiplist** 就是受到多层链表结构的启发而设计出来的。上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到 *O(logn)*。

<img src="C:\Users\10066\AppData\Roaming\Typora\typora-user-images\image-20240730000529760.png" alt="image-20240730000529760" style="zoom:50%;" />

- 多层索引的链表，上一层的元素都可以在下一层找到，最底层有全量数据
- 查询过程，先找第一层，发现数据在某两个节点中间再找下一层这两个节点中间的数，类似于二分，logn复杂度
- 插入过程，先查询到对应的位置，再随机判定是直接插入该层还是单独新加一层，如：插入48
  - <img src="C:\Users\10066\AppData\Roaming\Typora\typora-user-images\image-20240730000757047.png" alt="image-20240730000757047" style="zoom:33%;" />

- Redis 跳跃表默认允许最大的层数是 32
- Redis 跳表多了一个后退指针，是双向链表

##### 位图（Bitmap）

2.2新增的数据类型，是定义在字符串类型上的面向位的操作的集合（ byte 数组）。适合于设置 `2^32`个不同的位。

位图的最大优点是可以节省大量空间。一位一bit。

位图可以使用普通的 get/set 直接获取和设置整个位图的内容，也可以使用位图操作 getbit/setbit 等将 byte 数组看成「位数组」来处理。BITCOUNT 可以计算某个字节区间的true数量。

可以使用位图

- 统计用户签到天数
- 获得某个用户在线情况（对uid hash）

位图和布隆过滤器的区别

- 通常用于处理数字，字符串需要hash
- 位图在数据量过大情况下导致了内存浪费（一个位一个数据，布隆过滤器多个位一个数据）
- 都会有hash冲突误判，需要权衡位数还是hash函数个数对冲突的影响

##### Hyperloglog

2.8新增，海量数据的统计场景，有一定误差，可以统计UV PV

##### GEO

3.2新增，存储地理位置信息的场景，如百度地图

##### Stream

5.0新增，可以实现消息队列，相比正常的list，可自动生成全局唯一ID，且可以支持消费者组消费数据（同一消息可分发到单个消费者组或多个消费者组）

### Redis 快的原因

**Redis**采用的是基于内存的采用的是单进程单线程模型的 KV 数据库，由C语言编写，官方提供的数据是可以达到十万+的**QPS（每秒内查询次数）**

- 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。它的，数据存在内存中，类似于**HashMap**，**HashMap**的优势就是查找和操作的时间复杂度都是O(1)；
- 数据结构简单，对数据操作也简单，**Redis**中的数据结构是专门进行设计的，如跳表
- 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 **CPU**，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；
  - 严格来讲从 Redis6.0 之后并不是单线程，除了主线程外，它也有后台线程 在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 key 的删 除等等。
- 使用多路I/O复用模型，非阻塞IO；

虽然Redis是单线程的，但是可以在单机开多个Redis实例。

### IO多路复用模型

**Redis** 内部使用文件事件处理器 `file event handler`，Redis的IO多路复用模型（**IO多路复用就是Reactor 模式**）。

- **多路**：多个客户端连接（连接就是套接字描述符）
- **复用**：使用单进程就能够实现同时处理多个客户端的连接

Redis 服务器是一个事件驱动程序， 服务器处理的事件分为时间事件和文件事件两类。

- **文件事件**：Redis主进程中，主要处理客户端的连接请求与相应。
- **时间事件**：fork出的子进程中，处理如AOF持久化任务等。

由于Redis的文件事件是单进程，单线程模型，但是确保持着优秀的吞吐量，IO多路复用起到了主要作用。

文件事件是对套接字操作的抽象，每当一个套接字准备好执行连接应答、写入、读取、关闭等操作时，就会产生一个文件事件。因为一个服务器通常会连接多个套接字，所以多个文件事件有可能会并发地出现。

IO多路复用程序负责监听多个套接字并向文件事件分派器传送那些产生了事件的套接字。文件事件分派器接收IO多路复用程序传来的套接字，并根据套接字产生的事件的类型，调用相应的事件处理器。示例如图所示：

![img](https://ask.qcloudimg.com/http-save/yehe-4332331/b426855bd6502c00cf3f5461d822edc5.png)

Redis的IO多路复用程序的所有功能都是通过包装常见的select、poll、evport和kqueue这些IO多路复用函数库来实现的，每个IO多路复用函数库在Redis源码中都有对应的一个单独的文件。**采用I/O多路复用技术可以让单个线程高效处理多个连接请求**（尽量减少网络IO的时间消耗），将最耗时的socket读取，请求解析，写入单独外包出去，剩下的命令执行任然是由主线程串行执行和内存的数据交互。

Redis为每个IO多路复用函数库都实现了相同的API，所以IO多路复用程序的底层实现是可以互换的。

在Redis6.0中新增加了多线程的功能来**提高I/O读写性能**，他的主要实现思路是将**主线程的IO读写任务拆分给一组独立的线程去执行**，这样就可以使多个socket的读写并行化了。由于单线程的多路复用依然会进入阻塞状态，所以多线程会进一步利用多核CPU的优势，提高请求的处理效率，且仅对IO多线程，不会出现线程安全的问题。

[深入理解redis——Redis快的原因和IO多路复用深度解析 - 个人文章 - SegmentFault 思否](https://segmentfault.com/a/1190000041488709)

[详解redis网络IO模型 - jtea - 博客园 (cnblogs.com)](https://www.cnblogs.com/jtea/p/16969386.html)

### Redis 集群 高可用 持久化

集群的部署方式也就是**Redis cluster**，并且是主从同步读写分离，类似**Mysql**的主从同步，**Redis cluster** 支撑 N 个 **Redis master node**，每个**master node**都可以挂载多个 **slave node**。

这样整个 **Redis** 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 **master** 节点，每个 **master** 节点就能存放更多的数据了。

**Redis** 的数据 **全部存储** 在 **内存** 中，如果 **突然宕机**，数据就会全部丢失，因此必须有一套机制来保证 Redis 的数据不会因为故障而丢失，这种机制就是 Redis 的 **持久化机制**，它会将内存中的数据库状态 **保存到磁盘** 中。

#### 磁盘持久化

![图片](https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8iae8skGYoH6nlF88SxhhKEGxMt0TQKFFyL8X6epic3McpkxV8sibaj4zA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

1. 客户端向数据库 **发送写命令** *(数据在客户端的内存中)*
2. 数据库 **接收** 到客户端的 **写请求** *(数据在服务器的内存中)*
3. 数据库 **调用系统 API** 将数据写入磁盘 *(数据在内核缓冲区中)*
4. 操作系统将 **写缓冲区** 传输到 **磁盘控控制器** *(数据在磁盘缓存中)*
5. 操作系统的磁盘控制器将数据 **写入实际的物理媒介** 中 *(数据在磁盘中)*

如果我们故障仅仅涉及到 **软件层面** *(该进程被管理员终止或程序崩溃)* 并且没有接触到内核，那么在 *上述步骤 3* 成功返回之后，我们就认为成功了。即使进程崩溃，操作系统仍然会帮助我们把数据正确地写入磁盘。

如果我们考虑 **停电/ 火灾** 等 **更具灾难性** 的事情，那么只有在完成了第 **5** 步之后，才是安全的。

所以我们可以总结得出数据安全最重要的阶段是：**步骤三、四、五**，即：

- 数据库软件调用写操作将用户空间的缓冲区转移到内核缓冲区的频率是多少？
- 内核多久从缓冲区取数据刷新到磁盘控制器？
- 磁盘控制器多久把数据写入物理媒介一次？
- **注意：** 如果真的发生灾难性的事件，我们可以从上图的过程中看到，任何一步都可能被意外打断丢失，所以只能 **尽可能地保证** 数据的安全，这对于所有数据库来说都是一样的。

第三步可以通过Linux系统调用完成。

第四步通过发sync，该命令会 **强制** 内核将 **缓冲区** 写入 **磁盘**，但这是一个非常消耗性能的操作，每次调用都会 **阻塞等待** 直到设备报告 IO 完成，所以一般在生产环境的服务器中，**Redis** 通常是每隔 1s 左右执行一次 `fsync` 操作。

第五步完全无法控制。

#### RDB

RDB：**RDB** 持久化机制，是对 **Redis** 中的数据执行**周期性**的持久化。把**某一时刻的状态**以文件的形式进行**全量备份**到磁盘，这个快照文件就称为RDB文件(dump.rdb)。

Redis 是一个 **单线程** 的程序，这意味着，我们不仅仅要响应用户的请求，还需要进行内存快照（save命令，同步rdb持久化，会阻塞正常请求运行）。而后者要求 Redis 必须进行 IO 操作，这会严重拖累服务器的性能。

##### 使用系统多进程 COW(Copy On Write) 机制 | fork 函数

操作系统多进程 **COW(Copy On Write) 机制** 拯救了我们（bgsave命令，异步rdb持久化，子进程进行）。**Redis** 在持久化时会调用 `glibc` 的函数 `fork` 产生一个子进程，简单理解也就是基于当前进程 **复制** 了一个进程，主进程和子进程会共享内存里面的代码块和数据段：

![图片](https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8UK0nXA56QicaQ3gz516q8GiceGYZdDX6sbNOrPfk5J8TrBjMjW5RRSDg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

所以 **快照持久化** 可以完全交给 **子进程** 来处理，**父进程** 则继续 **处理客户端请求**。**子进程** 做数据持久化，它 **不会修改现有的内存数据结构**，它只是对数据结构进行遍历读取，然后序列化写到磁盘中。但是 **父进程** 不一样，它必须持续服务客户端请求，然后对 **内存数据结构进行不间断的修改**。

这个时候就会使用操作系统的 COW 机制来进行 **数据段页面** 的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复 制一份分离出来，然后 **对这个复制的页面进行修改**。这时 **子进程** 相应的页面是 **没有变化的**，还是进程产生时那一瞬间的数据。

子进程因为数据没有变化，它能看到的内存里的数据在进程产生的一瞬间就凝固了，再也不会改变，这也是为什么 **Redis** 的持久化 **叫「快照」的原因**。接下来子进程就可以非常安心的遍历数据了进行序列化写磁盘了。

##### 优点

他会生成多个数据文件，每个数据文件分别都代表了某一时刻**Redis**里面的数据，这种方式，有没有觉得很适合做**冷备**，完整的数据运维设置定时任务，定时同步到远端的服务器，比如阿里的云服务，这样一旦线上挂了，你想恢复多少分钟之前的数据，就去远端拷贝一份之前的数据就好了。

**RDB**对**Redis**的性能影响非常小，是因为在同步数据的时候他只是**fork**了一个子进程去做持久化的，而且他在数据恢复的时候速度比**AOF**来的快。

##### 缺点

**RDB**都是快照文件，都是默认五分钟甚至更久的时间才会生成一次，这意味着你这次同步到下次同步这中间五分钟的数据都很可能全部丢失掉。**AOF**则最多丢一秒的数据，**数据完整性**上高下立判。

还有就是**RDB**在生成数据快照的时候，如果文件很大，可能会影响CPU以及磁盘IO的性能，需要合理安排RDB的时机（比如避免秒杀场景）

#### AOF

AOF：**AOF** 机制对每条写入命令作为日志，以 **append-only** 的模式写入一个日志文件中，因为这个模式是只追加的方式，所以没有任何磁盘寻址的开销，所以很快，有点像Mysql中的**binlog**。

当 Redis 收到客户端修改指令后，会先进行参数校验、逻辑处理，如果没问题，就 **立即** 将该指令文本 **存储** 到 AOF 日志中，也就是说，**先执行指令再将日志存盘**。这一点不同于 `MySQL`、`LevelDB`、`HBase` 等存储引擎。

**Redis** 在长期运行的过程中，AOF 的日志会越变越长。如果实例宕机重启，重放整个 AOF 日志会非常耗时，导致长时间 Redis 无法对外提供服务。所以需要对 **AOF 日志 "瘦身"**。

**Redis** 提供了 `bgrewriteaof` 指令用于对 AOF 日志进行瘦身。其 **原理** 就是 **开辟一个子进程** **遍历** 转换成一系列 Redis 的操作指令，对里面大部分重复的命令或者可以合并的命令进行删减，再序列化到一个新的AOF文件中。序列化完毕后再将操作期间发生的 **增量 AOF 日志** 追加到这个新的 AOF 日志文件中，追加完毕后就立即替代旧的 AOF 日志文件了，瘦身工作就完成了。

##### 优点

**RDB**五分钟一次生成快照，但是**AOF**是一秒一次去通过一个后台的线程`fsync`操作，那最多丢这一秒的数据。

**AOF**在对日志文件进行操作的时候是以`append-only`的方式去写的，他只是追加的方式写数据，自然就少了很多磁盘寻址的开销了，写入性能惊人，文件也不容易破损。

**AOF**的日志是通过一个叫**非常可读**的方式记录的，这样的特性就适合做**灾难性数据误删除**的紧急恢复了，比如公司的实习生通过**flushall**清空了所有的数据，只要这个时候后台重写还没发生，你马上拷贝一份**AOF**日志文件，把最后一条**flushall**命令删了就完事了。

##### 缺点

一样的数据，**AOF**文件比**RDB**还要大。

AOF数据恢复时间比RDB大，因为要回放写操作。

#### 数据恢复

独用**RDB**你会丢失很多数据，你单独用**AOF**，你数据恢复没**RDB**来的快，所以在 Redis 重启的时候，可以先加载 `rdb` 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升（4.0后新增功能）。

两种方式都可以把**Redis**内存中的数据持久化到磁盘上，然后再将这些数据备份到别的地方去，**RDB**更适合做**冷备**，**AOF**更适合做**热备**，比如我杭州的某电商公司有这两个数据，我备份一份到我杭州的节点，再备份一个到上海的，就算发生无法避免的自然灾害，也不会两个地方都一起挂吧，这**灾备**也就是**异地容灾**，地球毁灭他没办法。

如果同时使用RDB和AOF两种持久化机制，那么在redis重启的时候，会优先使用AOF来重新构建数据，因为AOF中的数据更加完整。（可能是4.0支持混合恢复前的做法）

master节点，必须要使用持久化机制，防止master宕机恢复后覆盖slave上的数据。

#### 主从复制

master机器去写，数据同步给别的slave机器，他们都拿去读，分发掉大量的请求，而且扩容的时候还可以轻松实现水平扩容。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpw3kedn8KYhTFdutS1fDAiaql9jqeiboH8nWViaz8QibxEhGb32aNibCpFyGmFCkJtXte5pn5juAvziaCuw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

启动一台slave 的时候，他会发送一个**psync**命令给master ，如果是这个slave第一次连接到master，他会触发一个全量复制。master就会启动一个线程，执行bgsave生成**RDB**快照，还会把新的写请求都缓存在内存中，**RDB**文件生成后，master会将这个**RDB**发送给slave，slave拿到之后做的第一件事情就是写进本地的磁盘，然后加载进内存，然后master会把内存里面缓存的那些新命名都发给slave。

（1）从Redis服务器启动

![img](https://pic4.zhimg.com/80/v2-724629cf4662109af8b3a4556e05ff43_720w.webp)



（2）主Redis服务器收到SYNC命令后

![img](https://pic3.zhimg.com/80/v2-9e748fb5e672863bf3676d3b5d3cbe56_720w.webp)



（3）RDB持久化完成后

![img](https://pic2.zhimg.com/80/v2-aab33f7ddeb4b800575b875d29457045_720w.webp)



（4）复制初始化完成后

![img](https://pic4.zhimg.com/80/v2-de1601905a7f0152bde1787650f087f3_720w.webp)



①从数据库向主数据库发送sync(数据同步)命令。

②主数据库接收同步命令后，会执行bgsave，保存快照，创建一个RDB文件。

③当主数据库执行完保持快照后，会向从数据库发送RDB文件，而从数据库会接收并载入该文件。

④主数据库将这段时间新接收的命令写入replication buffer缓冲区，slave加载完RDB后将缓冲数据发给从服务器执行。

- replication buffer缓冲区用于主从之间实时传递写命令，每个slave都有单独的一个buffer，没有明确的大小限制，对于同步速度较慢的服务器，需要动态的更大的buffer

⑤以上处理完之后，之后主数据库每执行一个写命令，都会将被执行的写命令发送给从数据库。

注意：在Redis2.8之后，主从断开重连后会根据断开之前最新的命令偏移量进行增量复制

![img](https://pic1.zhimg.com/80/v2-41cc39ed2988bd8e2785ad63b093998c_720w.webp)

- psync命令的ID与主redisID相同
- 通过offset判断数据是否在repl_backlog_buffer中，若在则从offset开始发送后面的增量数据
  - repl_backlog_buffer是一个大小1MB（可设置）的环形缓冲区，保存写入命令，专门用于从节点重连后的增量恢复。若写满则会覆盖，所以需要判断offset是否已经被覆盖。
  - repl_backlog_buffer在主节点上只有一个，只存储最近写的命令
- 若不在则进行全量复制

#### 哨兵模式

哨兵必须用三个实例去保证自己的健壮性的，哨兵+主从并**不能保证数据不丢失**，但是可以保证集群的**高可用**。

当主节点出现故障时，由Redis 哨兵自动完成故障发现和转移，并通知应用方，实现高可用性。

![img](https://pic4.zhimg.com/80/v2-71a4f66da823854a35b82af3253e362b_720w.webp)

**Redis哨兵主要功能**

**（1）集群监控**：负责监控Redis master和slave进程是否正常工作

**（2）消息通知**：如果某个Redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员

**（3）故障转移**：如果master node挂掉了，会自动转移到slave node上

**（4）配置中心**：如果故障转移发生了，通知client客户端新的master地址

（1）哨兵机制建立了多个哨兵节点(进程)，共同监控数据节点的运行状况。

（2）同时哨兵节点之间也互相通信，交换对主从节点的监控状况。

（3）每隔1秒每个哨兵会向整个集群：Master主服务器+Slave从服务器+其他Sentinel（哨兵）进程，发送一次ping命令做一次心跳检测。

这个就是哨兵用来判断节点是否正常的重要依据，涉及两个新的概念**：主观下线和客观下线。**

哨兵的工作流程如下：

**主观下线(sdown)**

当某个哨兵心跳检测master超时后,则认定其sdown

**客观下线(odown)**

当认定sdown的哨兵数>=quorum（配置文件设置）时,则master下线事实最终成立,即odown（只有主节点有客观下线，从节点没有）

**选举哨兵leader**

判断出主管下线后，各哨兵协商，选举出一个leader，由其进行故障转移操作（只有哨兵leader才能进行故障转移）。

当一个Master服务器客观下线后，监控这个Master服务器的所有Sentinel将会选举出一个Sentinel Leader。并由Sentinel Leader对客观下线的Master进行故障转移。

每一个Sentinel节点都可以成为Leader，当一个Sentinel节点确认redis集群的主节点主观下线后，会请求其他Sentinel节点要求将自己选举为Leader。被请求的Sentinel节点如果没有同意过其他Sentinel节点的选举请求，则同意该请求(选举票数+1)，否则不同意。

如果一个Sentinel节点获得的选举票数达到Leader最低票数(`quorum`和`Sentinel节点数/2+1`的最大值)，则该Sentinel节点选举为Leader；否则重新进行选举。

**选举集群leader**

由哨兵leader进行集群leader选举。

- 根据从节点的优先级选择，优先选择优先级小的（可通过slave-priority配置）
- 节点优先级相同，查看主从复制的offset（复制偏移量），偏移量越大则同步的数据越多，优先级越高。
- offset相同，比较ID号，ID小的优先。

选举成功后由Redis的发布订阅模式，将新master IP端口等信息广播到所有节点。

**故障转移**

选择一个slave作为新的master, 并将其他节点设置为新master的slave (刚才已下线的老master的配置文件也会被设置slaveof)。当已下线的服务器重新上线后将复新的Master的数据。

**Redis在master宕机重新选举的期间，会出现短暂的阻塞，服务不可用的情况。（集群模式同样，会影响对应hash槽数据的写入）**

减少选举的影响：

- 配置优化，调整Redis集群的配置参数，加快选举过程
- 客户端重试
- 监控和警告

#### 数据分片（分区）

全量数据较大的场景下，单节点无法满足要求，需要数据分片，按照分片规则把数据分到若干个shard、partition当中按照分片规则把数据分到若干个shard、partition当中。

Redis采用虚拟哈希槽的分片方式。

<img src="C:\Users\10066\AppData\Roaming\Typora\typora-user-images\image-20240729215454856.png" alt="image-20240729215454856" style="zoom:50%;" />

- 首先预设虚拟槽，每个槽为一个hash值，每个node负责一定槽范围。为了保证数据hash均匀，槽的范围一般远远大于节点数。
  - Redis Cluster中预设虚拟槽的范围为0到16383，把16384槽按照节点数量进行平均分配。比如5个节点就每个节点负责16384/5个hash槽。
  - Redis Cluster的节点之间会共享消息，每个节点都会知道是哪个节点负责哪个范围内的数据槽
- 对每个key按照CRC16规则进行hash运算
- 把hash结果对16383进行取余
- 把余数发送给Redis节点
- 正常情况下，Redis客户端在启动时获取哈希槽到节点的映射关系，通过Hash算法将请求打到相应的节点
- 如果某个数据迁移或节点异常客户端未及时感知，则客户端也可以将请求打到任何一个Redis节点，节点接收到数据后再进行验证
  - 如果在自己管理的槽编号范围内，则把数据保存到数据槽中，然后返回执行结果。
  - 如果在自己管理的槽编号范围外，则会把数据发送给正确的节点，通过MOVED指令告知客户端进行重定向
    - 当集群在扩容时，会通过ASK指令到客户端进行重定向，客户端先发送ASKING到重定向后的新节点，再发送原指令（ASKING指令是因为可能当前扩容还没有完成，所以先临时授权该节点处理当前hash槽的指令）

无论数据规模大，还是小，Redis虚拟槽分区各个节点的负载，都会比较均衡 。而一致性哈希在大批量的数据场景下负载更加均衡，但是在数据规模小的场景下，会出现单位时间内某个节点完全空闲的情况出现。

#### 集群模式

由于哨兵模式始终只有一个Redis主机来接收和处理写请求，写操作还是受单机瓶颈影响，没有实现真正的分布式架构。所以我们采用集群模式来提高写性能。

**集群模式采用多主多从，每一个分区都是由一个Redis主机和多个从机组成，片区和片区之间是相互平行的。每一个分区保存的数据互不相同，为数据分片的结果。**

Redis Cluster集群采用了P2P的模式，完全去中心化。

首次启动的节点和被分配槽的节点都是主节点，从节点负责复制主节点槽信息和相关的数据。

![img](https://img-blog.csdnimg.cn/2020072714412735.png)



Redis集群常用的Gossip消息可分为：ping消息、pong消息、meet消息、fail消息：

- **meet消息** 会通知接收该消息的节点，发送节点要加入当前集群，接收者进行响应。
- **ping消息** 是集群中的节点定期向集群中其他节点（部分或全部）发送的连接检测以及信息交换请求，消息包含发送节点信息以及发送节点知道的其他节点信息。
- **pong消息** 是在节点接收到meet、ping消息后回复给发送节点的响应消息，告诉发送方本次通信正常，消息包含当前节点状态。
- **fail消息** 是在节点认为集群内另外某一节点下线后向集群内所有节点广播的消息。

Redis Cluster通过ping/pong可自动实现故障发现，不需要sentinel。

当某个节点判断另一个节点主观下线后，相应的节点状态会跟随消息在集群内传播。

当接受节点发现消息体中含有主观下线的节点状态且发送节点是主节点时，会在本地找到故障节点的ClusterNode结构，更新下线报告链表。

- 集群中的节点每次接收到其他节点的pfail状态，都会尝试触发客观下线。首先统计有效的下线报告数量，当下线报告数量大于槽主节点数量一半时，标记对应故障节点为客观下线状态。
- 向集群广播一条fail消息，通知所有的节点将故障节点标记为客观下线，fail消息的消息体只包含故障节点的ID。通知故障节点的从节点触发故障转移流程。
- 故障转移，在多个slave中进行新的leader选举，得票超过半数的slave将成为新的master。（Raft算法，投票过程同哨兵master选举，每个节点仅可投一张选票）
- 故障转移完成后，向集群广播自己的pong消息，通知集群内所有的节点当前从节点变为主节点并接管了故障主节点的槽信息。

##### 常见Redis集群架构

![img](https://images2018.cnblogs.com/blog/90573/201805/90573-20180515222110177-1033686810.jpg)

- Proxy做hash路由
- 分片实例之间相互独立，每组 一个master 实例和多个slave；
- 路由信息存放到第三方存储组件，如 zookeeper 或etcd
- 旁路组件探活

### Redis集群的一致性保证

Redis集群不能保证数据的强一致性。

- 主从节点之间使用了异步的方式来同步数据。

  - 可能在数据同步完成之前客户端进行读取。

- Redis可能造成数据丢失

  - 如果主节点在同步给slave之前挂了，这时新选举的master节点，该写操作就被永久消失了。

  - Redis集群出现集群脑裂。

    - redis的集群脑裂是指因为网络问题，导致redis master节点跟redis slave节点和sentinel集群处于不同的网络分区，此时因为sentinel集群无法感知到master的存在，所以将slave节点提升为master节点。此时存在两个不同的master节点，就像一个大脑分裂成了两个。

      集群脑裂问题中，如果客户端还在基于原来的master节点继续写入数据，那么新的master节点将无法同步这些数据，当网络问题解决之后，sentinel集群将原先的master节点降为slave节点，此时再从新的master中同步数据，将会造成大量的数据丢失。

    - 解决方案

      - Redis提供的配置参数，可以设置：要求至少有k个slave（min-slaves-to-write），数据复制和同步的slave ack延迟不能超过t秒（min-slaves-max-lag），如果不符合这个条件，那么master将不会接收任何请求。当有足够多个slave在合理时间响应时才可以进行写操作。
      
      - 由于master已经出现分区问题，所以不会及时的和其余slave进行数据复制，一旦slave复制数据和ack延时太长，就认为master宕机，那么就拒绝写请求，这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低到可控范围内。
      
      - `cluster-require-full-coverage`：设置为 `yes`，确保集群在部分节点不可用时拒绝写操作，避免数据不一致。
      
        `cluster-node-timeout`：设置节点故障检测的超时时间，确保快速检测并响应节点故障。
      
      - 客户端若遇到写失败也可进行重试操作，降低影响。

### Redis 过期策略

过期策略是为了保证有限的Redis内存始终保存的是热点Key。

定期删除+惰性删除

#### 定期删除

指的是redis默认是每隔100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意，这里可不是每隔100ms就遍历所有的设置过期时间的key，那样就是一场性能上的灾难。实际上redis是每隔100ms**随机抽取**一些key来检查和删除的。定期删除可能会导致很多过期key到了时间并没有被删除掉，这时候就需要惰性删除了。

#### 惰性删除

在获取某个key的时候，redis会检查一下 ，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除，返回null。

两种结合使用，就能够保证：一个key如果过期了，肯定会被删除了

但是，如果定期删除漏掉了很多过期key，然后也没及时去做查询，也就没走惰性删除，此时就可能会有大量过期key堆积在内存里，导致redis内存块耗尽
怎么办？

答案是：走内存淘汰机制。LRU、Random等

#### 内存回收

Redis内存使用到达maxmemory（可设置）时，触发，属于兜底策略。

- 不设置淘汰（默认），直接写入报错，但是允许查询
- 基于过期时间
  - LRU（最近最少使用，距离当前最后被用到的）
  - LFU（最少使用频率）
  - Random
- 基于所有数据
  - LRU
  - LFU
  - Random

### 布隆过滤器

**快速检测某个集合中是否包含某个值，时间复杂度 O(K)，K为Hash函数个数。**

布隆过滤器组成

- 一个位数组，boolen类型，初始值全为0（存储空间小）
- K个Hash函数

使用方法

- 插入：将key对每一个Hash函数求值（可以对hash结果取模映射到数组位），在位数组上将对应的位置置为1

  <img src="https://img-blog.csdnimg.cn/img_convert/a3e7d217ecb825e94bdc577a467eb29d.png" alt="增加元素.png" style="zoom:50%;" />

- 查找：将key对每一个Hash函数求值，如果位数组上所有对应位置都为1，则认为存在，否则不存在

  - 对**“存在某个值的判断”可能准确**，因为存在hash冲突
  - 对**“不存在某个值的判断”一定准确**

- 无法修改，无法删除，只能定期重置

### 缓存热点问题、big key问题、雪崩、穿透、击穿

#### 热点问题

热点 key，指的是在一段时间内，该 key 的访问量远远高于其他的 redis key， 导致大部分的访问流量在经过 proxy 分片之后，都集中访问到某一个 redis 实例上。

可通过redis-cli --hotkeys检查（4.0版本后），需扫描整个key。还可以通过monitor开启Redis监视器（但是很消耗性能）

解决方案

- 使用本地缓存
  - 可用Java的Hashmap，ConcurrentHashMap，第三方的Guava等。
  - 需要考虑本地缓存的一致性问题
- 利用分片算法，对key进行打散处理
  - 给hot key加上前缀或者后缀，把一个hotkey 的数量变成多个，不同用户hash到不同的key（服务器）上进行访问

#### Big key问题

数据量大的 key ，由于其数据大小远大于其他key，导致经过分片之后，某个具体存储这个 big key 的实例内存使用量远大于其他实例，造成，内存不足，拖累整个集群的使用。

- 内存分配不均，占用大量内存
- 操作大Key耗时较长，容易阻塞
- 对IO资源消耗过大

可以通过redis-cli --bigkeys检查大key

解决方案

- 对 big key 存储的数据压缩
- 对 big key 存储的数据 （big value）进行拆分，变成value1，value2… valueN。
- 使用合适的数据结构，如String存储的可以用Hash、Set等优化替代
- 优化业务逻辑

#### 穿透

指缓存和数据库中**都没有**的数据，而用户不断发起请求（可能是攻击）。

解决方案

- 对请求参数校验，防止非法请求
- 在缓存中将对应不存在Key的Value对写为null、位置错误、稍后重试这样的
- 限流，网关层限制同一ip的访问次数
- 布隆过滤器，用高效的数据结构和算法快速判断出你这个Key是否在数据库中存在，不存在你return就好了，存在你就去查了DB刷新KV再return。

#### 雪崩

指某个时刻大量缓存恰好同时到期失效，流量全部打到数据库，导致数据库崩了。

解决方案

- 在批量往Redis存数据的时候，把每个Key的失效时间都加个随机值就好了，这样可以保证数据不会在同一时间大面积失效
- 数据预热（或者预加载）
- 多级缓存，本地缓存、nginx缓存等
- 设置热点数据永不过期
- 服务限流、熔断、降级
- 设置高可用的Redis集群
- 加互斥锁，在缓存失效时，同一时间只能由一个请求来构建缓存，其他请求阻塞

#### 击穿

雪崩的特殊情况，一个热点Key，在不停的扛着大并发，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，导致数据库崩了。

解决方案

- 数据预热（或者预加载）
- 设置热点数据永不过期
- 多级缓存，本地缓存、nginx缓存等
- 服务限流、熔断、降级
- 设置高可用的Redis集群
- 加互斥锁，在缓存失效时，同一时间只能由一个请求来构建缓存，其他请求阻塞

总体：可以采取限流、降级等方式，控制突发的高并发场景。

### 缓存和数据库之间的一致性

#### 三个缓存模式

- 旁路缓存
  - 读：读缓存，读数据库，更新缓存
  - 写：写数据库，删缓存
    - 为什么是删缓存不是更新缓存
      - 同时写时，可能造成缓存与数据库数据不一致
      - 写多读少时，没必要实时更新缓存，浪费性能
    - 为什么先写数据库
      - 同时写时，可能造成缓存与数据库数据不一致
- 读写穿透
  - 读：读缓存，读数据库，更新缓存
  - 写：写数据库，写缓存
  - 与旁路缓存的区别：client不直接操作缓存与数据库，通过Cache-Provider进行封装。
- 异步写
  - 读：读缓存，读数据库，更新缓存
  - 写：写缓存，异步更新数据库
  - 提高可用性，降低一致性

#### 数据库和缓存一致性保证

- 延时双删
  - 删除缓存，更新数据库，再删缓存
  - 第二次删的目的：防止在写数据库时，新的读请求将老数据更新到缓存中。
  - 如果第二次删除失败
    - 删除缓存重试机制，为了防止脏数据，必须保证第二次删除成功
    - 可以采用消息队列进行重试
- binlog异步删缓存
  - 将mysql的更新日志binlog发送到消息队列中（可以使用阿里的canal）
  - 订阅消息队列，根据更新log删除缓存（也可以更新L1 L2缓存，或者更新es）
  - 使用rocketMQ的at least once模式（至少消费一次，以及同步模式），通过ack，保证消息必须消费成功，并返回成功ack，否则就不断重试，重试达到上限人工介入。
    - At most once：最多一次。消息可能会丢失，但绝不会重复传输。一般都是一些对消息可靠性要求不太高的监控场景使用
    - At least once：最少一次。消息绝不会丢失，但可能会重复传输。
    - Exactly once：恰好一次。每条消息肯定会被传输一次且仅传输一次。
  - 主从数据库的情况
    - 主从DB同步存在延时时间。如果删除缓存之后，数据同步到备库之前已经有请求过来， 会从备库中读到脏数据并更新缓存
    - 解决方案：对所有从库，消息队列每收到一次更新binlog，都删除一次缓存

#### 为什么是删缓存，不是更新缓存

- 性能问题，Lazy 计算的思想，频繁更新的代价，是否经常被访问
- 一致性问题，可能两个线程同时写一个数据，A比B先写数据库但是由于网络问题A比B后更新缓存，造成数据库与缓存中的数据不一致。

#### 二级缓存、三级缓存

二级缓存：一级L1 - 进程内存， 二级L2 - redis

为了防止热点数据，以及集中式缓存雪崩、击穿等，可以再加一层本地缓存，将数据存在应用程序内存中，提高访问速度。

本地缓存一般适合于缓存只读、量少、高频率访问的数据。如秒杀商品数据。

保持数据一致性：通过binlog消息队列，或者其他广播模式的发布订阅，保持各个一级缓存的数据一致性。
<img src="https://img-blog.csdnimg.cn/20210605172721406.png" alt="在这里插入图片描述" style="zoom:67%;" />

三级缓存：一级 - 进程内存， 二级 - redis，三级 - Nginx

[GitHub - AobingJava/JavaFamily: 【Java面试+Java学习指南】 一份涵盖大部分Java程序员所需要掌握的核心知识。](https://github.com/AobingJava/JavaFamily)

### 常见面试题

#### Redis使用场景

- 缓存
- 分布式锁
- 计数器
  - 单线程，没有锁，高效
  - 如：文章点赞数量，日活统计
- 消息队列
  - 消息发布：列表的lpush，插入列表头部，如果列表不存在则创建
  - 消息接收：rpop，获取尾部数据
  - <img src="C:\Users\10066\AppData\Roaming\Typora\typora-user-images\image-20240731114223461.png" alt="image-20240731114223461" style="zoom:50%;" />
- 其他
  - 抽奖、秒杀、排行榜（zset）、鉴权、限流等

#### Redis和Memcache

相同点

- 都是基于内存的数据库，操作速度很快
- 都有对应的缓存过期策略

不同

- Redis数据类型更丰富（9种），Memcache只支持k-v一种
- Redis支持发布订阅、Lua脚本等，Memcache不支持
- Redis支持数据持久化，Memcache不支持，宕机直接丢失数据
- Redis处理数据是单线程，Memcache多线程
- Redis支持集群模式，数据分片和负载均衡，Memcache不支持分布式，需要用户手动操作分片和负载均衡。

#### Redis事务

Redis对事务仅支持原子操作，不被打断，但是如果出现问题不能回滚

#### Lua脚本

- 保证了Redis执行的原子性，一个脚本内的命令原子执行，执行期间会阻塞其他命令，避免并发问题
- 减少网络交互次数，提高性能
- 时间限制为5s，超过则报错（可通过lua-time-limit限制）
- 尽量简短，避免阻塞过长时间

#### Pipeline

正常情况下执行多条命令需要和Redis服务器多次交互，Pipeline可以把多条命令封装在一起发给服务端，服务端依次执行命令，并缓存结果，所有命令执行完后再一起返回给客户端。

<img src="C:\Users\10066\AppData\Roaming\Typora\typora-user-images\image-20240730192841678.png" alt="image-20240730192841678" style="zoom:50%;" />

- 节省RTT（往返时间）
- 减少上下文切换的开销，服务端从网络读数据写数据时都需要通过系统调用，涉及到用户态和内核态的切换。
- 官方推荐最多10k命令，防止客户端等待过长时间
- Pipeline不能保证原子性

#### 发布订阅功能

发布：publish topic message

订阅：subscribe topic

Redis保存了映射关系，topic -> 消费者，当有消息发布时，Redis不会保存消息（AOF和RDB都不保存），只会将消息写入缓冲区，查找映射关系并进行转发。

所以当Redis宕机或者消费者消费过慢缓冲区溢出时，都会造成消息丢失的情况。

Redis的哨兵集群以及Redis之间的实例通信，都是采用发布订阅功能。

#### Redis 分布式锁

- 采用 setnx + expire
  - setnx - set if not exist，当且仅当 key 不存在时，将 key 的值设为 value。若 key 已经存在，则不做任何动作。
  - expire 为 key 设置生存时间，当 key 过期时，它会被自动删除。
- 问题：setnx + expire 不原子
  - 解一：使用 set 时，同时设置过期时间，不单独使用 expire。set key "value" EX 100 NX
  - 解二：使用 lua 脚本，将加锁的命令放在 lua 脚本中原子性的执行。
    - 在 eval 命令执行 Lua 代码的时候，Lua 代码将被当成一个命令去执行，并且直到 eval 命令执行完成，Redis才会执行其他命令。
  - 注意：删除锁的时候要判断该锁是否是自己上的，不能删除了别人新设置的锁，可将线程ID或者唯一标识ID当做锁 value。
- 问题：锁过期释放，业务没执行完
  - Redisson，watch dog后台线程，每10秒检查一下，如果业务持锁，就会继续延长锁的生存时间
  - Redission支持可重入锁的操作，获取锁时通过唯一标识ID判断是否属于当前线程，如果是则增加计数器，释放锁时则减少计数器，当计时器为0时才真正释放锁。
- 问题：redis master-slave 架构的主从异步复制导致，master实例宕机或主从同步未完成，其他从节点还没有上锁，数据不一致
  - 使用多机版分布式锁Redission + Redlock
    - Redlock：Redis官方推出的锁，为了保证分布式锁的可靠性。
    - 其要求不能只在一个redis实例上创建锁，应该是在大多数redis节点上都成功创建锁，才能算这个整体的RedLock加锁成功。但是对性能影响严重，不推荐使用
  - redis 分布式锁可能导致的数据不一致性，建议使用业务补偿的方式去弥补。
- 单点故障问题，Redis单机时可能宕机影响锁的情况。

#### 统计UV

使用HyperLogLog统计UV、PV。

HyperLogLog是一种基数估算算法，用于快速计算一个集合中不同元素数量的值

占用极小的内存（12KB），极小的误差（不到1%）

PFADD key，添加元素。PFCOUNT key，返回不重复元素近似数量。

原理是通过Hash函数和位图，以及估算公式进行估算。

#### 常见Redis客户端

- Jedis
- Lettuce
- Redisson

#### mset mget

- 和Pipeline一样属于批处理命令
- 原子性处理（Pipeline非原子）
- 没有Pipeline灵活（Pipeline可以又读又写）

#### 复制延迟可能的原因

- 网络原因，主从网络
- 大量写操作，主节点负载较高
- 从节点处理速度过慢，复制缓冲区溢出































